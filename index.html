<!DOCTYPE HTML>
<html>
	<head>
		<title>Full-Stack, GPU-based Acceleration of Deep Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />		
		<link rel="stylesheet" type="text/css" href="data/stylesheet.css">
		<link rel="stylesheet" href="data/academicons-1.9.1/css/academicons.css" />
		<link rel="stylesheet" href="data/fontawesome-free-6.0.0-web/css/all.css">
	</head>
	<body>
		
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>										
			<p style="text-align:center">
				<name>Full-Stack, GPU-based Acceleration of Deep Learning</name>			
			  </p>			  
			  <p style="text-align:center">
				<img src="data/nvidia.png" alt="Nvidia" style="width:100px;max-width:15%;min-width:75px">
					  <!-- <name>Nvidia</name>-->
				</p>			
			<tr style="padding:0px">
			<td style="padding:0px">				
			  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr style="padding:0px">  
				  <td style="width:80%;vertical-align:top">					
				      <br/>
					  <br>
					  <p style="text-align:left">
	  					  <font size="+2">Full-Stack, GPU-based Acceleration of Deep Learning</font>
					  </p>
					  This tutorial focuses on describing techniques to allow deep learning practitioners to accelerate the training and inference of large deep networks while also reducing memory requirements across a spectrum of off-the-shelf hardware for important applications such as autonomous driving and large language models. Topics include, but are not limited to:
                      <ul>
					    <li>Deep learning specialized hardware overview. We review the architecture of the most used deep learning acceleration hardware, including the main computational processors and memory modules.</li>
                        <li>How deep learning is performed on this hardware. We cover aspects of algorithmic intensity and an overview of theoretical aspects of computing. Attendees will learn how to estimate processing time and latency by looking only at hardware specs and the network architecture.</li>
                        <li>Best practices for acceleration. We provide an overview of best practices for designing efficient neural networks including channel number selection, compute heavy operations, or reduction operations among others.</li>
                        <li>Existing tools for model acceleration. In this part we will focus on existing tools to accelerate a trained neural network on GPU devices. We will particularly discuss operation folding, TensorRT, ONNX graph optimization, sparsity.</li>
                        <li>Research overview of recent techniques. In the last part, we will focus on recent advanced techniques for post training model optimization including pruning, quantization, model distillation or NAS among others.</li>
                    </ul>
					  <br>
				  </td>
				  
				</tr>
			  </tbody>
			</table>
		  <hr>		  
	  	  <hr>
			<div>
				<h2><font size="+2">Schedule</font> </h2>								
				<table>
					<tr>
					  <td>13:30</td><td>13:35</td><td>Opening Remarks</td>
					  </tr><tr>
					  <td>13:35</td><td>14:15</td><td>Jason Clemons</td><td>foundations of GPU architecture, hardware perspective.</td>
					  </tr>
					  <tr>
					  <td>14:15</td><td>15:00</td><td>Pavlo Molchanov</td><td>Algorithmic views on GPU compute, how to maximize performance</td>
					  </tr>
					  <tr>
					  <td>15:00</td><td>15:30</td><td>Coffee Break</td><td></td>
					  </tr><tr>
					   <td>15:30</td><td>16:15</td><td>MAying Shen</td><td>Sparsity in DNN and model compression</td>
					</tr>
					   <tr>
					   <td>16:15</td><td>17:00</td><td>Hongxu (Danny) Yin</td><td>Recent trends on transformer acceleration, data efficiency and security,</td>
					</tr>
				  </table>
								
			</div>		

			<hr>
		
		</td>
	</tr>
</table>


      

      
    </section>
  </div>


</body>  			  		  
</html>